{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bd2c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "\n",
    "\n",
    "from operator import sub\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "import torch\n",
    "import shutil\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import esim_torch\n",
    "from upsampling import *\n",
    "from esim_torch import esim_torch\n",
    "from upsampling.utils import Upsampler\n",
    "import esim_torch as et\n",
    "#from general_event_generator.events_generator import EventGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Developed by: Eman Ehab Nasef\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "class VideoProcessor:\n",
    "\n",
    "    def __get_video_info(self):\n",
    "\n",
    "            try:\n",
    "\n",
    "                \"\"\"Calculates the proberities of a video file using ffprobe. \"\"\"\n",
    "\n",
    "                properties_cmd = [\"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v:0\", \"-show_entries\",\n",
    "                        \"stream=height,width,nb_frames,duration\", \"-of\", \"json\", \n",
    "                        self.video_path]\n",
    "                \n",
    "                result = subprocess.run(properties_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n",
    "                                        text=True)\n",
    "\n",
    "                # Parse the JSON output to get the duration.\n",
    "                \n",
    "                output_json = json.loads(result.stdout)\n",
    "                \n",
    "                width = int(output_json[\"streams\"][0][\"width\"])\n",
    "                height = int(output_json[\"streams\"][0][\"height\"])\n",
    "                duration = float(output_json[\"streams\"][0][\"duration\"])\n",
    "                frame_count = int(output_json[\"streams\"][0][\"nb_frames\"])\n",
    "                fps = int(frame_count/duration)\n",
    "                return width, height, duration, frame_count, fps\n",
    "            \n",
    "            except:\n",
    "                print(\"No VIDEO output found\")\n",
    "                return None\n",
    "\n",
    "    def __init__(self, video_path, cut_part_duration = 2, resize_scale=0.3, debug=False):\n",
    "        \"\"\" \n",
    "        Class that preprocess video, devide it into chuncks and \n",
    "        \"\"\"\n",
    "        self.video_path = video_path\n",
    "        self.cut_part_duration = cut_part_duration\n",
    "        self.resize_scale = resize_scale\n",
    "        self.debug = debug\n",
    "\n",
    "        self.video_name = os.path.basename(self.video_path).split(\".\")[0]\n",
    "        \n",
    "        self.orignal_folder_path = f\"{os.path.dirname(self.video_path)}/orginal/\"\n",
    "        self.upsampling_dir =f\"{os.path.dirname(self.video_path)}/upsampled/\"\n",
    "        self.events_dir = f\"{os.path.dirname(self.video_path)}/events/\"\n",
    "\n",
    "        if not os.path.exists(self.orignal_folder_path):\n",
    "                os.mkdir( self.orignal_folder_path)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"video_name: {self.video_name}\")\n",
    "            print(f\"orignal_folder_path: {self.orignal_folder_path}\")\n",
    "        \n",
    "        try:\n",
    "            self.width, self.height, self.video_duration, self.total_frames, \\\n",
    "                                        self.fps = self.__get_video_info()\n",
    "            self.num_cut_parts = int(self.video_duration) // int(self.cut_part_duration)\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting video info: {e}\")\n",
    "            print(\"---\")\n",
    "\n",
    "            self.width, self.height, self.video_duration, self.total_frames, \\\n",
    "                self.fps = None, None, None, None, None\n",
    "            print(self.width, self.height, self.video_duration, self.total_frames, \\\n",
    "                self.fps) \n",
    "   \n",
    "    def cut_resize_video(self):\n",
    "        \"\"\"\n",
    "            Use this function as the first step to cut and resize the video.\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(self.num_cut_parts):\n",
    "            if i == 0:\n",
    "                cut_part_start_time = 0\n",
    "            else:\n",
    "                cut_part_start_time = i + self.cut_part_duration\n",
    "                \n",
    "            cut_part_end_time = cut_part_start_time + self.cut_part_duration\n",
    "\n",
    "            # Handle last video duration\n",
    "            if (cut_part_start_time + self.cut_part_duration) > self.video_duration :\n",
    "                cut_part_end_time = self.video_duration\n",
    "                \n",
    "                \n",
    "            #create a cut sequence folder\n",
    "            \n",
    "            video_parts_path = os.path.join(self.orignal_folder_path, f\"seq_{i}\")\n",
    "\n",
    "\n",
    "            if not os.path.exists(video_parts_path):\n",
    "                os.mkdir( video_parts_path)\n",
    "\n",
    "            # Create the output file path for the cut part\n",
    "            cut_part_output_path = os.path.join(video_parts_path, f\"{self.video_name}_part_{i}.mp4\")\n",
    "\n",
    "            if self.debug:\n",
    "\n",
    "                print(f\"cut_part_start_time: {cut_part_start_time}\", cut_part_start_time, end=\" - \")\n",
    "                print(f\"cut_part_end_time: {cut_part_end_time}\", end=\" - \")\n",
    "                print(f\"video_parts_path: {video_parts_path}\", end=\" - \")\n",
    "                print(f\"cut_part_output_path: {cut_part_output_path}\", end=\" - \")\n",
    "                print(\"----------------------------------------------\")\n",
    "                \n",
    "\n",
    "            # Cut and resize the video\n",
    "            cut_resize_cmd = [\"ffmpeg\", \"-i\", self.video_path, \n",
    "                            \"-ss\", str(cut_part_start_time), \n",
    "                            \"-t\", str(cut_part_end_time),\n",
    "                            \"-vf\", f\"scale={self.resize_scale}*iw:{self.resize_scale}*ih\",\n",
    "                            cut_part_output_path]\n",
    "            subprocess.run(cut_resize_cmd)\n",
    "            \n",
    "\n",
    "            # Check if the video if empty through number of frames\n",
    "            print(self.video_duration)\n",
    "            if self.__get_video_info() is None:\n",
    "\n",
    "                print(cut_part_output_path)\n",
    "                print(\" video cut error\")\n",
    "                shutil.rmtree(video_parts_path)\n",
    "                print(f\"cut_part_end_time: {cut_part_end_time} \")\n",
    "                print(f\"The whole video duration is {self.duration}\")\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                # Add a subprocess to extract video frames from the new video chuncks\n",
    "\n",
    "                cut_part_frames_path = os.path.join(video_parts_path, \"imgs\")\n",
    "                \n",
    "                if not os.path.exists(cut_part_frames_path):\n",
    "                    os.mkdir( cut_part_frames_path)\n",
    "\n",
    "                vid2frame_cmd = [\"ffmpeg\",\n",
    "                        \"-i\",\n",
    "                        cut_part_output_path,\n",
    "                        f\"{cut_part_frames_path}/%08d.png\",]\n",
    "\n",
    "                subprocess.run(vid2frame_cmd)\n",
    "\n",
    "                # Create fps.txt file\n",
    "                fps_txt_path = os.path.join(video_parts_path, \"fps.txt\")\n",
    "                \n",
    "                with open(fps_txt_path, \"w\") as f:\n",
    "                    f.write(str(self.fps))\n",
    "\n",
    "\n",
    "    def upsample_video(self):\n",
    "            \"\"\"\n",
    "                Use this function as the 2nd step to upsample video chuncks using interpolation between frames.\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                upsampler = Upsampler(input_dir=self.orignal_folder_path, output_dir=self.upsampling_dir)\n",
    "                \n",
    "                upsampler.upsample()\n",
    "            except Exception as e:\n",
    "                print(f\"{e}, please check upsampling code!\")\n",
    "\n",
    "\n",
    "    def generate_timestamps(self, frame_rate):\n",
    "        \"\"\"\n",
    "            use those functions at the last to generate timestamps for the upsamppled frames. \n",
    "        \"\"\"\n",
    "        for folder in folders:\n",
    "            images = sorted(\n",
    "                [f for f in listdir( args.input_folder+folder+\"/imgs\") if f.endswith('.png')])\n",
    "            time_path = args.input_folder + folder \n",
    "\n",
    "            print('Will write file: {} with framerate: {} Hz'.format(\\\n",
    "                time_path + '/timestamps.txt',  args.framerate))\n",
    "            \n",
    "            stamp_nanoseconds = 1\n",
    "            dt_nanoseconds = int((1.0 /  args.framerate) * 1e9 )\n",
    "\n",
    "\n",
    "            with open(join(time_path, 'timestamps.txt'), 'w') as f:\n",
    "                for image_path in images:\n",
    "\n",
    "                    f.write('{}\\n'.format(int(stamp_nanoseconds)))\n",
    "                    stamp_nanoseconds += dt_nanoseconds\n",
    "                f.close()\n",
    "        \n",
    "\n",
    "\n",
    "    def __is_valid_dir(subdirs, files):\n",
    "        return len(subdirs) == 1 and len(files) == 1 and \"timestamps.txt\" in files and \"imgs\" in subdirs\n",
    "\n",
    "\n",
    "    def __process_dir(outdir, indir, contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns):\n",
    "        print(f\"Processing folder {indir}... Generating events in {outdir}\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # constructor\n",
    "        esim = et.ESIM(contrast_threshold_negative,\n",
    "                            contrast_threshold_positive,\n",
    "                            refractory_period_ns)\n",
    "\n",
    "        timestamps = np.genfromtxt(os.path.join(indir, \"timestamps.txt\"), dtype=\"float64\")\n",
    "        timestamps_ns = (timestamps * 1e9).astype(\"int64\")\n",
    "        timestamps_ns = torch.from_numpy(timestamps_ns).cuda()\n",
    "\n",
    "        image_files = sorted(glob.glob(os.path.join(indir, \"imgs\", \"*.png\")))\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=len(image_files)-1)\n",
    "        num_events = 0\n",
    "\n",
    "        counter = 0\n",
    "        for image_file, timestamp_ns in zip(image_files, timestamps_ns):\n",
    "            image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            log_image = np.log(image.astype(\"float32\") / 255 + 1e-5)\n",
    "            log_image = torch.from_numpy(log_image).cuda()\n",
    "\n",
    "            sub_events = esim.forward(log_image, timestamp_ns)\n",
    "\n",
    "            # for the first image, no events are generated, so this needs to be skipped\n",
    "            if sub_events is None:\n",
    "                continue\n",
    "\n",
    "            sub_events = {k: v.cpu() for k, v in sub_events.items()}    \n",
    "            num_events += len(sub_events['t'])\n",
    "\n",
    "\n",
    "    def generate_events(self, contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns):\n",
    "         \n",
    "        input_dir = self.upsampling_dir\n",
    "        output_dir = self.events_dir\n",
    "\n",
    "        if not os.path.exists(input_dir):\n",
    "            raise   Exception(f\"{Exception}: Please use 1st and 2nd steps firist to generate events!\")\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "\n",
    "        print(f\"Generating events with cn={contrast_threshold_negative}, cp={contrast_threshold_positive} \\\n",
    "              and rp={refractory_period_ns}\")\n",
    "\n",
    "        for path, subdirs, files in os.walk(input_dir):\n",
    "            if self.__is_valid_dir(subdirs, files):\n",
    "                output_folder = os.path.join(output_dir, os.path.relpath(path, input_dir))\n",
    "\n",
    "                self.__process_dir(output_folder, path)\n",
    "        \n",
    "                \n",
    "\"\"\"\n",
    "    Developed by: Eman Ehab Nasef\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_name: video_0003\n",
      "orignal_folder_path: ./Archive-code/Test_vidCutResizer/orginal/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 11:43:20.528786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:20.530779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:20.531540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:20.533295: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-04 11:43:20.534695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:20.535822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:20.536800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:22.318784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:22.319474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:22.320091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-04 11:43:22.320758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5766 MB memory:  -> device: 0, name: GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequence number ./Archive-code/Test_vidCutResizer/orginal/seq_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImageSequence:   0%|          | 0/749 [00:00<?, ?it/s]2023-11-04 11:43:26.071511: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2023-11-04 11:43:29.432484: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8500\n",
      "ImageSequence:   2%|▏         | 13/749 [00:41<34:30,  2.81s/it] "
     ]
    }
   ],
   "source": [
    "v_pth = \"./Archive-code/Test_vidCutResizer/video_0003.mp4\"\n",
    "vcr = VideoProcessor(v_pth, cut_part_duration=7, debug=True)\n",
    "vcr.upsample_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_name: video_0003\n",
      "orignal_folder_path: ./Archive-code/Test_vidCutResizer/orginal/\n",
      "Generating events with cn=0.2, cp=0.2               and rp=0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__is_valid_dir() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb Cell 3\u001b[0m line \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m refractory_period_ns \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m vcr \u001b[39m=\u001b[39m VideoProcessor(v_pth, cut_part_duration\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, debug\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m vcr\u001b[39m.\u001b[39;49mgenerate_events(contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns)\n",
      "\u001b[1;32m/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb Cell 3\u001b[0m line \u001b[0;36mVideoProcessor.generate_events\u001b[0;34m(self, contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=244'>245</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating events with cn=\u001b[39m\u001b[39m{\u001b[39;00mcontrast_threshold_negative\u001b[39m}\u001b[39;00m\u001b[39m, cp=\u001b[39m\u001b[39m{\u001b[39;00mcontrast_threshold_positive\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=245'>246</a>\u001b[0m \u001b[39m      and rp=\u001b[39m\u001b[39m{\u001b[39;00mrefractory_period_ns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=247'>248</a>\u001b[0m \u001b[39mfor\u001b[39;00m path, subdirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(input_dir):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__is_valid_dir(subdirs, files):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=249'>250</a>\u001b[0m         output_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mrelpath(path, input_dir))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y104sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__process_dir(output_folder, path)\n",
      "\u001b[0;31mTypeError\u001b[0m: __is_valid_dir() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "framerate = 1200\n",
    "v_pth = \"./Archive-code/Test_vidCutResizer/video_0003.mp4\"\n",
    "contrast_threshold_negative = 0.2\n",
    "contrast_threshold_positive = 0.2\n",
    "refractory_period_ns = 0\n",
    "\n",
    "\n",
    "vcr = VideoProcessor(v_pth, cut_part_duration=10, debug=True)\n",
    "vcr.generate_events(contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framerate = 1200\n",
    "v_pth = \"./Archive-code/Test_vidCutResizer/video_0003.mp4\"\n",
    "contrast_threshold_negative = 0.2\n",
    "contrast_threshold_positive = 0.2\n",
    "refractory_period_ns = 0\n",
    "\n",
    "\n",
    "input_dir = f\"{os.path.dirname(self.video_path)}/upsampled/\"\n",
    "output_dir = f\"{os.path.dirname(self.video_path)}/events/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_name: video_0003\n",
      "orignal_folder_path: ./Archive-code/Test_vidCutResizer/orginal/\n"
     ]
    }
   ],
   "source": [
    "v_pth = \"./Archive-code/Test_vidCutResizer/video_0003.mp4\"\n",
    "vcr = VideoProcessor(v_pth, cut_part_duration=10, debug=True)\n",
    "#vcr.cut_resize_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequence number ./Archive-code/Test_vidCutResizer/orginal/seq_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ImageSequence:   4%|▍         | 17/449 [00:33<14:12,  1.97s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sampling_output_dir \u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(v_pth)\u001b[39m}\u001b[39;00m\u001b[39m/upsampled/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m upsampler \u001b[39m=\u001b[39m Upsampler(input_dir\u001b[39m=\u001b[39msampling_input_dir, output_dir\u001b[39m=\u001b[39msampling_output_dir)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/eman/Documents/AUC/ESIM/rpg_vid2e/final-generating_events_pipeline.ipynb#Y103sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m upsampler\u001b[39m.\u001b[39;49mupsample()\n",
      "File \u001b[0;32m~/Documents/AUC/ESIM/rpg_vid2e/upsampling/utils/upsampler.py:39\u001b[0m, in \u001b[0;36mUpsampler.upsample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m dest_imgs_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdest_dir, reldirpath, imgs_dirname)\n\u001b[1;32m     38\u001b[0m dest_timestamps_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdest_dir, reldirpath, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timestamps_filename)\n\u001b[0;32m---> 39\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupsample_sequence(sequence, dest_imgs_dir, dest_timestamps_filepath)\n",
      "File \u001b[0;32m~/Documents/AUC/ESIM/rpg_vid2e/upsampling/utils/upsampler.py:51\u001b[0m, in \u001b[0;36mUpsampler.upsample_sequence\u001b[0;34m(self, sequence, dest_imgs_dir, dest_timestamps_filepath)\u001b[0m\n\u001b[1;32m     48\u001b[0m I1 \u001b[39m=\u001b[39m img_pair[\u001b[39m1\u001b[39m][\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     49\u001b[0m t0, t1 \u001b[39m=\u001b[39m time_pair\n\u001b[0;32m---> 51\u001b[0m total_frames, total_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upsample_adaptive(I0, I1, t0, t1)\n\u001b[1;32m     52\u001b[0m total_frames \u001b[39m=\u001b[39m [I0[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m total_frames\n\u001b[1;32m     53\u001b[0m timestamps \u001b[39m=\u001b[39m [t0] \u001b[39m+\u001b[39m total_timestamps\n",
      "File \u001b[0;32m~/Documents/AUC/ESIM/rpg_vid2e/upsampling/utils/upsampler.py:84\u001b[0m, in \u001b[0;36mUpsampler._upsample_adaptive\u001b[0;34m(self, I0, I1, t0, t1, num_bisections)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mreturn\u001b[39;00m [image[\u001b[39m0\u001b[39m]], [(t0 \u001b[39m+\u001b[39m t1) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m     83\u001b[0m left_images, left_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upsample_adaptive(I0, image, t0, (t0\u001b[39m+\u001b[39mt1)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, num_bisections\u001b[39m=\u001b[39mnum_bisections\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m right_images, right_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upsample_adaptive(image, I1, (t0\u001b[39m+\u001b[39;49mt1)\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m, t1, num_bisections\u001b[39m=\u001b[39;49mnum_bisections\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m timestamps \u001b[39m=\u001b[39m left_timestamps \u001b[39m+\u001b[39m [(t0\u001b[39m+\u001b[39mt1)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m right_timestamps\n\u001b[1;32m     86\u001b[0m images \u001b[39m=\u001b[39m left_images \u001b[39m+\u001b[39m [image[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m right_images\n",
      "File \u001b[0;32m~/Documents/AUC/ESIM/rpg_vid2e/upsampling/utils/upsampler.py:84\u001b[0m, in \u001b[0;36mUpsampler._upsample_adaptive\u001b[0;34m(self, I0, I1, t0, t1, num_bisections)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mreturn\u001b[39;00m [image[\u001b[39m0\u001b[39m]], [(t0 \u001b[39m+\u001b[39m t1) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m     83\u001b[0m left_images, left_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upsample_adaptive(I0, image, t0, (t0\u001b[39m+\u001b[39mt1)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, num_bisections\u001b[39m=\u001b[39mnum_bisections\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m right_images, right_timestamps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upsample_adaptive(image, I1, (t0\u001b[39m+\u001b[39;49mt1)\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m, t1, num_bisections\u001b[39m=\u001b[39;49mnum_bisections\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m timestamps \u001b[39m=\u001b[39m left_timestamps \u001b[39m+\u001b[39m [(t0\u001b[39m+\u001b[39mt1)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m right_timestamps\n\u001b[1;32m     86\u001b[0m images \u001b[39m=\u001b[39m left_images \u001b[39m+\u001b[39m [image[\u001b[39m0\u001b[39m]] \u001b[39m+\u001b[39m right_images\n",
      "File \u001b[0;32m~/Documents/AUC/ESIM/rpg_vid2e/upsampling/utils/upsampler.py:73\u001b[0m, in \u001b[0;36mUpsampler._upsample_adaptive\u001b[0;34m(self, I0, I1, t0, t1, num_bisections)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m [], []\n\u001b[1;32m     72\u001b[0m dt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_dt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull(shape\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,), fill_value\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 73\u001b[0m image, F_0_1, F_1_0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolator\u001b[39m.\u001b[39;49minterpolate(I0, I1, dt)\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m num_bisections \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     76\u001b[0m     flow_mag_0_1_max \u001b[39m=\u001b[39m ((F_0_1 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m.5\u001b[39m)\u001b[39m.\u001b[39mmax()\n",
      "File \u001b[0;32m~/Documents/AUC/ESIM/rpg_vid2e/upsampling/utils/interpolator.py:104\u001b[0m, in \u001b[0;36mInterpolator.interpolate\u001b[0;34m(self, x0, x1, dt)\u001b[0m\n\u001b[1;32m    101\u001b[0m   x1, _ \u001b[39m=\u001b[39m _pad_to_align(x1, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_align)\n\u001b[1;32m    103\u001b[0m inputs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mx0\u001b[39m\u001b[39m'\u001b[39m: x0, \u001b[39m'\u001b[39m\u001b[39mx1\u001b[39m\u001b[39m'\u001b[39m: x1, \u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m: dt[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, np\u001b[39m.\u001b[39mnewaxis]}\n\u001b[0;32m--> 104\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(inputs, training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    105\u001b[0m forward_flow \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39mforward_flow_pyramid\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    106\u001b[0m backward_flow \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39mbackward_flow_pyramid\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/saved_model/load.py:664\u001b[0m, in \u001b[0;36m_call_attribute\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_attribute\u001b[39m(instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 664\u001b[0m   \u001b[39mreturn\u001b[39;00m instance\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    922\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    925\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    926\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    927\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   3040\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1964\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    592\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    593\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    594\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    595\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    596\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    597\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.venvs/event2img/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampling_input_dir = f\"{os.path.dirname(v_pth)}/orginal/\"\n",
    "sampling_output_dir =f\"{os.path.dirname(v_pth)}/upsampled/\"\n",
    "\n",
    "upsampler = Upsampler(input_dir=sampling_input_dir, output_dir=sampling_output_dir)\n",
    "\n",
    "upsampler.upsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimestampsUpsampler:\n",
    "\n",
    "    def __init__(self, upsample_dir, framerate):\n",
    "        self.upsample_dir = upsample_dir\n",
    "        self.framerate = framerate\n",
    "\n",
    "    def generate_timestamps(self):\n",
    "\n",
    "        for i, seq_path in enumerate(self.upsample_dir):\n",
    "\n",
    "            images = sorted(\n",
    "                [f for f in os.listdir(seq_path +\"/imgs\") if f.endswith('.png')])\n",
    "\n",
    "            print('Will write file: {} with framerate: {} Hz'.format(\\\n",
    "                seq_path + '/timestamps.txt',  self.framerate))\n",
    "            \n",
    "            stamp_nanoseconds = 1\n",
    "            dt_nanoseconds = int((1.0 /  self.framerate) * 1e9 )\n",
    "\n",
    "\n",
    "            with open(os.path.join(seq_path, 'timestamps.txt'), 'w') as f:\n",
    "                for image_path in images:\n",
    "\n",
    "                    f.write('{}\\n'.format(int(stamp_nanoseconds)))\n",
    "                    stamp_nanoseconds += dt_nanoseconds\n",
    "                f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25439406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventGenerator:\n",
    "    def __init__(self, contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns):\n",
    "        self.contrast_threshold_negative = contrast_threshold_negative\n",
    "        self.contrast_threshold_positive = contrast_threshold_positive\n",
    "        self.refractory_period_ns = refractory_period_ns\n",
    "        self.esim = esim_torch.ESIM(contrast_threshold_negative,\n",
    "                                    contrast_threshold_positive,\n",
    "                                    refractory_period_ns)\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_valid_dir(subdirs, files):\n",
    "        return len(subdirs) == 1 and len(files) == 1 and \"timestamps.txt\" in files and \"imgs\" in subdirs\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_dir(subdirs, files):\n",
    "        return len(subdirs) == 1 and len(files) == 1 and \"timestamps.txt\" in files and \"imgs\" in subdirs\n",
    "\n",
    "    def process_dir(self, outdir, indir):\n",
    "        print(f\"Processing folder {indir}... Generating events in {outdir}\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        timestamps = np.genfromtxt(os.path.join(indir, \"timestamps.txt\"), dtype=\"float64\")\n",
    "        timestamps_ns = (timestamps * 1e9).astype(\"int64\")\n",
    "        timestamps_ns = torch.from_numpy(timestamps_ns).cuda()\n",
    "\n",
    "        image_files = sorted(glob.glob(os.path.join(indir, \"imgs\", \"*.png\")))\n",
    "        \n",
    "        pbar = tqdm.tqdm(total=len(image_files)-1)\n",
    "        num_events = 0\n",
    "\n",
    "        counter = 0\n",
    "        for image_file, timestamp_ns in zip(image_files, timestamps_ns):\n",
    "            image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "            log_image = np.log(image.astype(\"float32\") / 255 + 1e-5)\n",
    "            log_image = torch.from_numpy(log_image).cuda()\n",
    "\n",
    "            sub_events = self.esim.forward(log_image, timestamp_ns)\n",
    "\n",
    "            if sub_events is None:\n",
    "                continue\n",
    "\n",
    "            sub_events = {k: v.cpu() for k, v in sub_events.items()}    \n",
    "            num_events += len(sub_events['t'])\n",
    "     \n",
    "            np.savez(os.path.join(outdir, \"%010d.npz\" % counter), **sub_events)\n",
    "            pbar.set_description(f\"Num events generated: {num_events}\")\n",
    "            pbar.update(1)\n",
    "            counter += 1\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventGenerationManager:\n",
    "    \n",
    "    def __init__(self, video_path, contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns):\n",
    "        \n",
    "        self.input_dir = f\"{os.path.dirname(video_path)}/upsampled/\"\n",
    "        self.output_dir = f\"{os.path.dirname(video_path)}/events/\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.mkdir( self.output_dir)\n",
    "        \n",
    "        self.contrast_threshold_negative = contrast_threshold_negative\n",
    "        self.contrast_threshold_positive = contrast_threshold_positive\n",
    "        self.refractory_period_ns = refractory_period_ns\n",
    "    \n",
    "    def process_directories(self, event_generator):\n",
    "        for path, subdirs, files in os.walk(self.input_dir):\n",
    "            if EventGenerator.is_valid_dir(subdirs, files):\n",
    "                output_folder = os.path.join(self.output_dir, os.path.relpath(path, self.input_dir))\n",
    "                event_generator.process_dir(output_folder, path)\n",
    "\n",
    "    def start_event_generation(self):\n",
    "        event_gen = EventGenerator(self.contrast_threshold_negative, self.contrast_threshold_positive, self.refractory_period_ns)\n",
    "        print(f\"Generating events with cn={event_gen.contrast_threshold_negative}, cp={event_gen.contrast_threshold_positive} and rp={event_gen.refractory_period_ns}\")\n",
    "        self.process_directories(event_gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating events with cn=0.2, cp=0.2 and rp=0\n",
      "Processing folder ./example2/upsampled/dirname_does_not_matter... Generating events in ./example2/events/dirname_does_not_matter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num events generated: 806137: 100%|██████████| 64/64 [00:00<00:00, 140.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder ./example2/upsampled/seq2... Generating events in ./example2/events/seq2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num events generated: 69442: : 17it [00:00, 175.47it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder ./example2/upsampled/seq0... Generating events in ./example2/events/seq0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num events generated: 300538: : 65it [00:00, 159.52it/s]                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder ./example2/upsampled/seq1... Generating events in ./example2/events/seq1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Num events generated: 12337: : 65it [00:00, 180.54it/s]                      \n"
     ]
    }
   ],
   "source": [
    "# Define parameters directly in the script\n",
    "v_pth = \"./Archive-code/Test_vidCutResizer/video_0003.mp4\"\n",
    "contrast_threshold_negative = 0.2\n",
    "contrast_threshold_positive = 0.2\n",
    "refractory_period_ns = 0\n",
    "\n",
    "manager = EventGenerationManager(v_pth, contrast_threshold_negative, contrast_threshold_positive, refractory_period_ns)\n",
    "manager.start_event_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ns = np.load(\"./example2/events/seq0/0000000049.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1405d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "y\n",
      "t\n",
      "p\n"
     ]
    }
   ],
   "source": [
    "for key in event_ns.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "path = './example2/events/seq0/'\n",
    "\n",
    "npz_files = sorted(os.listdir(path))\n",
    "\n",
    "for file in npz_files:\n",
    "    frame_data = np.load(path+str(file))\n",
    "\n",
    "    # Extract x, y, t, and p arrays from the npz file\n",
    "    x = frame_data['x']\n",
    "    y = frame_data['y']\n",
    "    t = frame_data['t']\n",
    "    p = frame_data['p']\n",
    "\n",
    "    # Convert event-based data to frame\n",
    "    try:\n",
    "        max_x = np.max(x)\n",
    "        max_y = np.max(y)\n",
    "        frame = np.zeros((max_y + 1, max_x + 1), dtype=np.uint8)\n",
    "\n",
    "        # Iterate through the events and update the frame\n",
    "        for i in range(len(x)):\n",
    "            frame[y[i], x[i]] = 255 if p[i] else 0\n",
    "            cv2.imshow('Frame', frame)\n",
    "            cv2.waitKey(0.9)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb74036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e97ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e6306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vid2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
